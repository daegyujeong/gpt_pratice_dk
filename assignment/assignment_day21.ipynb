{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{\n",
                            "  documents: VectorStoreRetriever(tags=['FAISS', 'CacheBackedEmbeddings'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x299410810>),\n",
                            "  question: RunnablePassthrough(),\n",
                            "  chat_history: RunnableLambda(...)\n",
                            "}\n",
                            "| ChatPromptTemplate(input_variables=['chat_history', 'context', 'question'], input_types={'chat_history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"\\n            Given the following extracted parts of a long document and a question, create a final answer. \\n            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\\n            ------\\n            {context}\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])\n",
                            "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.1, openai_api_key='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', openai_proxy='')"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain.chat_models import ChatOpenAI\n",
                "from langchain.document_loaders import UnstructuredFileLoader\n",
                "from langchain.text_splitter import CharacterTextSplitter\n",
                "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
                "from langchain.vectorstores import FAISS\n",
                "from langchain.storage import LocalFileStore\n",
                "from langchain.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
                "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
                "from langchain.memory import ConversationBufferMemory\n",
                "\n",
                "llm = ChatOpenAI(\n",
                "    temperature=0.1,\n",
                ")\n",
                "\n",
                "cache_dir = LocalFileStore(\"./.cache/\")\n",
                "\n",
                "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
                "    separator=\"\\n\",\n",
                "    chunk_size=600,\n",
                "    chunk_overlap=100,\n",
                ")\n",
                "loader = UnstructuredFileLoader(\"../files/chapter_three.txt\")\n",
                "\n",
                "docs = loader.load_and_split(text_splitter=splitter)\n",
                "\n",
                "embeddings = OpenAIEmbeddings()\n",
                "\n",
                "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
                "\n",
                "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
                "\n",
                "retriever = vectorstore.as_retriever()\n",
                "\n",
                "memory = ConversationBufferMemory(llm=llm,\n",
                "                                  max_token_limit=200,\n",
                "                                  return_messages=True,\n",
                "                                  memory_key=\"chat_history\")\n",
                "\n",
                "\n",
                "\n",
                "def load_memory(_):\n",
                "    return memory.load_memory_variables({})[\"chat_history\"]\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "prompt = ChatPromptTemplate.from_messages(\n",
                "    [\n",
                "        (\n",
                "            \"system\",\n",
                "            \"\"\"\n",
                "            Given the following extracted parts of a long document and a question, create a final answer. \n",
                "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
                "            ------\n",
                "            {context}\n",
                "            \"\"\",\n",
                "        ),\n",
                "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
                "        (\"human\", \"{question}\"),\n",
                "    ]\n",
                ")\n",
                "\n",
                "chain = {\n",
                "    \"context\": retriever,\n",
                "    \"question\": RunnablePassthrough(),\n",
                "    \"chat_history\": load_memory,\n",
                "}  | prompt | llm\n",
                "\n",
                "chain\n",
                "def invoke_chain(question):\n",
                "    result = chain.invoke(question).content\n",
                "    memory.save_context(\n",
                "        {'input': question},\n",
                "        {'output': result},\n",
                "    )\n",
                "    print(result)\n",
                "    \n",
                "invoke_chain('Is Aaronson guilty?')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "KeyError",
                    "evalue": "'context'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chain\u001b[39m.\u001b[39;49minvoke(\u001b[39m\"\u001b[39;49m\u001b[39mIs Aaronson guilty?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
                        "File \u001b[0;32m~/Documents/gpt_pratice_dk/env/lib/python3.11/site-packages/langchain/schema/runnable/base.py:1213\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1212\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1213\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1214\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1215\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1216\u001b[0m             patch_config(\n\u001b[1;32m   1217\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1218\u001b[0m             ),\n\u001b[1;32m   1219\u001b[0m         )\n\u001b[1;32m   1220\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
                        "File \u001b[0;32m~/Documents/gpt_pratice_dk/env/lib/python3.11/site-packages/langchain/schema/prompt_template.py:60\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m---> 60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[1;32m     61\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_prompt(\n\u001b[1;32m     62\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[1;32m     63\u001b[0m         ),\n\u001b[1;32m     64\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m     65\u001b[0m         config,\n\u001b[1;32m     66\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     67\u001b[0m     )\n",
                        "File \u001b[0;32m~/Documents/gpt_pratice_dk/env/lib/python3.11/site-packages/langchain/schema/runnable/base.py:715\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    708\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    709\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    710\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    711\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[1;32m    712\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    713\u001b[0m )\n\u001b[1;32m    714\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 715\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[1;32m    716\u001b[0m         func, \u001b[39minput\u001b[39;49m, config, run_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    717\u001b[0m     )\n\u001b[1;32m    718\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    719\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
                        "File \u001b[0;32m~/Documents/gpt_pratice_dk/env/lib/python3.11/site-packages/langchain/schema/runnable/config.py:308\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    307\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 308\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                        "File \u001b[0;32m~/Documents/gpt_pratice_dk/env/lib/python3.11/site-packages/langchain/schema/prompt_template.py:62\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m     61\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[0;32m---> 62\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[1;32m     63\u001b[0m         ),\n\u001b[1;32m     64\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m     65\u001b[0m         config,\n\u001b[1;32m     66\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m     )\n",
                        "File \u001b[0;32m~/Documents/gpt_pratice_dk/env/lib/python3.11/site-packages/langchain/schema/prompt_template.py:62\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m     61\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[0;32m---> 62\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[1;32m     63\u001b[0m         ),\n\u001b[1;32m     64\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m     65\u001b[0m         config,\n\u001b[1;32m     66\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m     )\n",
                        "\u001b[0;31mKeyError\u001b[0m: 'context'"
                    ]
                }
            ],
            "source": [
                "chain.invoke(\"Is Aaronson guilty?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
